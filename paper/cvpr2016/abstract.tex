%!TEX root = top.tex
\begin{abstract}
Incorporating multi-scale features in fully convolutional neural networks (FCNs) has been a key element to achieving state-of-the-art performance on semantic image segmentation.
One common way to extract multi-scale features is to feed multiple resized input images to a shared deep network and then merge the resulting features for pixel-wise classification.
In this work, we propose an attention mechanism that learns to softly weight the multi-scale features at each pixel location.
We adapt a state-of-the-art semantic image segmentation model, which we jointly train with multi-scale input images and the attention model. 
%We jointly train the network and the attention model which learns to softly weight the multi-scale features, and show that it outperforms average- and max-pooling over scales.
The proposed attention model not only outperforms average- and max-pooling, but allows us to diagnostically visualize the importance of features at different positions and scales. %, allowing us to qualitatively understand the model mechanism for every example.
Moreover, we show that adding extra supervision to the output at each scale is essential to achieving excellent performance when merging multi-scale features. 
We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.

%Incorporating multi-scale features in deep convolutional neural networks (DCNNs) has been a key to achieving state-of-art performance on semantic image segmentation benchmarks. One way to extract multi-scale features is to feed several resized input images to a shared deep network and then merge the resulting multi-scale features for pixel-wise classification. In this work, we adapt a state-of-art semantic image segmentation model with multi-scale input images. We jointly train the network and an attention model which learns to softly weight the multi-scale features, and show that it outperforms average- and max-pooling over scales. The proposed attention model allows us to diagnostically visualize the importance of features at different positions and scales. Moreover, we show that adding extra supervision to the output of a DCNN at each scale is essential to achieve excellent performance when merging multi-scale features. We demonstrate the effectiveness of our model with extensive experiments on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.

%% In this work, we focus on models based on two types of networks to exploit multi-scale features. The first type, skip-net, takes use of the features from different layers of the network for classification, while the second type, share-net, applies multi-scale input images to a shared network. In particular, we adapt a state-of-art semantic segmentation model to be a type of skip-net. The proposed model is enriched by an attention model which learns to softly weight the multi-scale features, and by introducing extra supervision to the output of DCNN for each scale. We show that the proposed model outperforms strong baselines and its share-net counterpart. Moreover, we are able to visualize the importance of features at different positions and different scales by the attention model. We demonstrate the effectiveness of our model on three challenging datasets. %: PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.

%% In particular, we extend the publicly available model DeepLab with multi-scale inputs and attention model. We jointly learn the DCNN component as well as the attention model that softly weights the multi-scale features. The proposed model outperforms DeepLab-MSc, avoids the two-step training process employed by the models based on skip-net, and unveils the black box inherited by the models based on share-net, which are unable to visualize the importance of features at different scales. Furthermore, we show that our proposed model can achieve better performance over strong baselines on three challenging datasets, including PASCAL-Person-Part, PASCAL VOC 2012 and a subset of MS-COCO 2014.
\end{abstract}
